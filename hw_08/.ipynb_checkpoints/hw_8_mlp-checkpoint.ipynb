{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Multilayer perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the 8th practical session of CS233 - Introduction to Machine Learning.\n",
    "\n",
    "The goal of today's session is to get you familiar neural networks, backpropragation and training. To do that, we will not use any external libraries and we will code from scratch a vanilla multilayer perceptron. \n",
    "\n",
    "After this session, we will use standard deep learning libraries, as coding a neural network from scratch is not that easy, as you will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions introduce non-linearities in our network. They are a fundamental ingredient in a neural networks, as they allow them to learn highly non-linear mappings between input and target domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will look only at the classical Sigmoid activation function, defined as \n",
    "\n",
    "\\begin{align}\n",
    "S(z) = \\frac{1}{1 + e^{-z}}.\n",
    "\\end{align}\n",
    "\n",
    "In order to train our networks, we will need to compute its derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dS}{dz}(z) = \\frac{d}{dz} \\left( \\frac{1}{1 + e^{-z}} \\right) = -1 (1 + e^{-z})^{-2} (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2} = S(z)(1-S(z))\n",
    "\\end{align}\n",
    "\n",
    "Implement this function and its gradient below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def forward(z):\n",
    "        return ...\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, you can test your implementation by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-10,10,1000)\n",
    "S = Sigmoid.forward(z)\n",
    "dS_dz = Sigmoid.gradient(z)\n",
    "\n",
    "plt.plot(z, S)\n",
    "plt.title('Sigmoid')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(z, dS_dz)\n",
    "plt.title('Derivative Sigmoid')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very popular activation function, Rectified Linear Unit (ReLU), is defined as \n",
    "\n",
    "\\begin{align}\n",
    "ReLU(z) = \\begin{cases}\n",
    "               0               & z<0\\\\\n",
    "               z               & z\\geq 0\\\\\n",
    "           \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "and its (sub-)derivative reads:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d ReLU}{dz}(z) = \\begin{cases}\n",
    "               0               & z<0\\\\\n",
    "               1               & z\\geq 0\\\\\n",
    "           \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Implement this function and its derivative below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    @staticmethod\n",
    "    def forward(z):\n",
    "        return ...\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, you can test your implementation by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-10,10,1000)\n",
    "T = ReLU.forward(z)\n",
    "dT_dz = ReLU.gradient(z)\n",
    "\n",
    "plt.plot(z, T)\n",
    "plt.title('ReLU')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(z, dT_dz)\n",
    "plt.title('Derivative of ReLU')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define our first neural network through the Network class.\n",
    "\n",
    "We implemented for you the constructor of the class, which given a specification of the dimensions of each layer $l$ and a list of per-layer activation functions, initializes the weights $w^{(l)}$ and $b^{(l)}$ of our network as\n",
    "\n",
    "\\begin{align}\n",
    "b^{(l)} _i &= 0 \\, \\, \\, i=1,\\dots,I \\\\\n",
    "w^{(l)} _{ji} &= \\frac{\\mathcal N(0,1)}{\\sqrt{J}}  \\, \\, \\, i=1,\\dots,I \\, \\, \\,  j=1,\\dots,J\n",
    "\\end{align}\n",
    "\n",
    "where $I,J$ are the number of neurons respectively at the output/input of $l$-th layer.\n",
    "\n",
    "You will have to implement the forward pass of the network, which is how it generates its output $Y$.\n",
    "Inputs are fed into the the network are multiplied with the weights and shifted along the biases for every layer $l$.\n",
    "The mathematics of the forward pass are the same for every layer $l$ \n",
    "\n",
    "\\begin{align}\n",
    "z^{(l+1)}  &= a^{(l)} \\cdot w^{(l)} + b^{(l)} \\\\\n",
    "a^{(l+1)} &= f(z^{(l+1)}),\n",
    "\\end{align}\n",
    "\n",
    "where $f$ denotes our activation function.\n",
    "\n",
    "Implement the forward pass in the method $\\texttt{feed_forward}$ below, which given input $x$ returns $z$ and $a$, two dictionaries containing outputs and activations for every layer. Keeping track of computed quantities will be useful for training the model. \n",
    "\n",
    "Method $\\texttt{predict}$, which we have implemented for you, is just a wrapper around $\\texttt{feed_forward}$ yielding the output of our network $Y = a^{(L)}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, dimensions, activations):\n",
    "        \"\"\"\n",
    "        :param dimensions: list of dimensions of the neural net. (input, hidden layer, ... ,hidden layer, output)\n",
    "        :param activations: list of activation functions. Must contain N-1 activation function, where N = len(dimensions).\n",
    "        \n",
    "        Example of one hidden layer with\n",
    "        - 2 inputs\n",
    "        - 10 hidden nodes\n",
    "        - 5 outputs\n",
    "        layers -->    [1,        2,          3]\n",
    "        ----------------------------------------\n",
    "        dimensions =  (2,     10,          5)\n",
    "        activations = (      Sigmoid,      Sigmoid)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = None\n",
    "        self.learning_rate = None\n",
    "        \n",
    "        # Weights and biases are initiated by index.\n",
    "        self.w = {}\n",
    "        self.b = {}\n",
    "\n",
    "        # Activations are also initiated by index. \n",
    "        self.activations = {}\n",
    "        for i in range(len(dimensions) - 1):\n",
    "            self.w[i + 1] = np.random.randn(dimensions[i], dimensions[i + 1]) / np.sqrt(dimensions[i])\n",
    "            self.b[i + 1] = np.zeros(dimensions[i + 1])\n",
    "            self.activations[i + 2] = activations[i]\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        \"\"\"\n",
    "        Execute a forward feed through the network.\n",
    "        :param x: (array) Batch of input data vectors.\n",
    "        :return: (tpl) Node outputs and activations per layer. The numbering of the output is equivalent to the layer numbers.\n",
    "        \"\"\"\n",
    "\n",
    "        # w(x) + b\n",
    "        z = {}\n",
    "\n",
    "        # activations: f(z)\n",
    "        a = {1: x}  # First layer has no activations as input, so we consider input itself as the first activation.\n",
    "        \n",
    "        for i in range(1, self.n_layers):\n",
    "            # current layer = i\n",
    "            # activation layer = i + 1\n",
    "            z[i + 1] = ...\n",
    "            a[i + 1] = ...\n",
    "\n",
    "        return z, a\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (array) Containing parameters\n",
    "        :return: (array) A 2D array of shape (n_cases, n_classes).\n",
    "        \"\"\"\n",
    "        _, a = self.feed_forward(x)\n",
    "        return a[self.n_layers]\n",
    "    \n",
    "\n",
    "    def back_prop(self, z, a, y_true):\n",
    "        \"\"\"\n",
    "        The input dicts keys represent the layers of the net.\n",
    "        a = { 1: x,\n",
    "              2: f(w1(x) + b1)\n",
    "              3: f(w2(a2) + b2)\n",
    "              }\n",
    "        :param z: (dict) w(x) + b\n",
    "        :param a: (dict) f(z)\n",
    "        :param y_true: (array) One hot encoded truth vector.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Determine partial derivative and delta for the output layer.\n",
    "        y_pred = a[self.n_layers]\n",
    "        delta = self.loss_function.gradient(y_true, y_pred) * self.activations[self.n_layers].gradient(y_pred)\n",
    "        \n",
    "        dw = np.dot(a[self.n_layers - 1].T, delta)\n",
    "\n",
    "        update_params = {\n",
    "            self.n_layers - 1: (dw, delta)\n",
    "        }\n",
    "\n",
    "        # Determine partial derivative and delta for the rest of the layers.\n",
    "        # Each iteration requires the delta from the previous layer, propagating backwards.\n",
    "        for i in reversed(range(2, self.n_layers)):\n",
    "            delta = np.dot(delta, self.w[i].T) * self.activations[i].gradient(z[i])\n",
    "            dw = np.dot(a[i - 1].T, delta)\n",
    "            update_params[i - 1] = (dw, delta)\n",
    "        \n",
    "        # finally update weights and biases\n",
    "        for k, v in update_params.items():\n",
    "            self.update_w_b(k, v[0], v[1])\n",
    "\n",
    "    def update_w_b(self, index, dw, delta):\n",
    "        \"\"\"\n",
    "        Update weights and biases.\n",
    "        :param index: (int) Number of the layer\n",
    "        :param dw: (array) Partial derivatives\n",
    "        :param delta: (array) Delta error.\n",
    "        \"\"\"\n",
    "\n",
    "        self.w[index] -= self.learning_rate * dw\n",
    "        self.b[index] -= self.learning_rate * np.mean(delta, 0)\n",
    "\n",
    "    def fit(self, x, y_true, loss, epochs, batch_size, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        :param x: (array) Containing parameters\n",
    "        :param y_true: (array) Containing one hot encoded labels.\n",
    "        :param loss: Loss class (MSE, CrossEntropy etc.)\n",
    "        :param epochs: (int) Number of epochs.\n",
    "        :param batch_size: (int)\n",
    "        :param learning_rate: (flt)\n",
    "        \"\"\"\n",
    "        if not x.shape[0] == y_true.shape[0]:\n",
    "            raise ValueError(\"Length of x and y arrays don't match\")\n",
    "        # Initiate the loss object with the final activation function\n",
    "        self.loss_function = loss\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Shuffle the data\n",
    "            indices = np.arange(x.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            x_ = x[indices]\n",
    "            y_ = y_true[indices]\n",
    "\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                k = j * batch_size\n",
    "                l = (j + 1) * batch_size\n",
    "                z, a = self.feed_forward(x_[k:l])\n",
    "                self.back_prop(z, a, y_[k:l])\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                _, a = self.feed_forward(x)\n",
    "                print(\"Loss at epoch {}: {}\".format(i + 1, self.loss_function.loss(y_true, a[self.n_layers])))\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now created your first neural network, that can produce outputs based on inputs. \n",
    "To make sure everything is right, you can run the cell below and make sure the dictionary structure looks like this:\n",
    "\n",
    "z:\n",
    "{\n",
    "\t2: \"z values of the hidden layer\", \n",
    "\t3: \"z values of the output layer\"\n",
    "}\n",
    "\n",
    "a:\n",
    "{\n",
    "\t1: \"inputs x\",\n",
    "\t2: \"activations in the hidden layer\",\n",
    "\t3: \"activations in the output layer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize  network structure\n",
    "nn = Network((1, 1, 1), (Sigmoid, Sigmoid))\n",
    "x = np.random.rand(1,1)\n",
    "z,a = nn.feed_forward(x)\n",
    "print(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions map the predictions of our network into a real number, measuring the error in our predictions.\n",
    "\n",
    "In this first session we will take the Mean Squared Error (MSE) as our loss function.\n",
    "\n",
    "Let $Y_n \\in \\mathbb R ^ K$ be the prediction generated by our network for $n$-th data sample, and $Y  \\in \\mathbb R ^{N \\times K} $ be the matrix accumulating N predictions generated from a sample of N data points.\n",
    "\n",
    "Similarly let $\\hat Y \\in \\mathbb R ^{N \\times K}$ be the matrix accumulating N observed values $\\hat Y_n \\in \\mathbb R ^ K$.\n",
    "\n",
    "We then define the MSE of the predictor as\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal L _{\\text{MSE}} =\\frac {1}{NK} \\sum _{n=1}^{N} \\sum _{k=1}^{K}\\left(Y_{n}^k-{\\hat {Y}_{n}^k}\\right)^{2}.\n",
    "\\end{align}\n",
    "\n",
    "We can compute its derivative w.r.t. predictions $Y_i$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal L _{\\text{MSE}} }{\\partial Y_{n} ^k} =\\frac {2}{NK} \\left(Y_{n}^k-{\\hat {Y}_{n} ^k}\\right).\n",
    "\\end{align}\n",
    "\n",
    "and consequntly, re-writing it in vectorial form\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal L _{\\text{MSE}} }{\\partial Y} =\\frac {2}{NK} (Y-{\\hat {Y}}).\n",
    "\\end{align}\n",
    "\n",
    "Implement this loss function and its derivative in vectorial form below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: (array) One hot encoded truth vector.\n",
    "        :param y_pred: (array) Prediction vector\n",
    "        :return: (flt)\n",
    "        \"\"\"\n",
    "        return ...\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(y_true, y_pred):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function $\\mathcal L$ gives us information about the prediction error of the model. Our goal is to minimize the loss function by optimizing the weights and biases. \n",
    "\n",
    "We know from calculus that if we take the partial derivative of the loss function with respect to a certain weight $w^{(l)}_{ij}$ we get the rate of change of $\\mathcal L$ in that direction. \n",
    "\n",
    "The opposite direction of $\\frac{\\partial \\mathcal L }{\\partial w^{(l)}_{ij}}$ will then point towards the direction that minimizes the output of the loss function.\n",
    "We can therefore, choosing a step size $\\eta$, reduce the loss function by updating our weight as follows\n",
    "\\begin{align}\n",
    "w^{(l)}_{ij} = w^{(l)}_{ij} - \\eta \\frac{\\partial \\mathcal L }{\\partial w^{(l)}_{ij}}\n",
    "\\end{align}\n",
    "This weight optimization is called gradient descent and is done with every weight and bias of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the partial derivative of $\\mathcal L$ with respect to a certain weight, we can apply the chain rule in differentiation\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}\n",
    "\\end{align}\n",
    "\n",
    "In practice this allows us to break the problem down whitin each layer. We call the whole process backpropagation.\n",
    "\n",
    "\n",
    "In order to understand backpropagation, we will take a step back and consider a very simple neural network with one hidden layer taking scalar input $x \\in \\mathbb R$ and outputting scalar $y \\in \\mathbb R$\n",
    "\n",
    "\\begin{align}\n",
    "z^{(2)} &= x w^{(1)} + b^{(1)} \\\\\n",
    "a^{(2)} &= S(z^{(2)}) \\\\\n",
    "z^{(3)} &= a^{(2)} w^{(2)} + b^{(2)} \\\\\n",
    "y &= S(z^{(3)}) \\\\\n",
    "\\mathcal L &= (y-\\hat y)^2\n",
    "\\end{align}\n",
    "\n",
    "Using the chain rule, compute derivatives of $\\mathcal L$ with respect to all weights and biases:\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal L}{\\partial y} &= 2 (y-\\hat y) \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial z ^ {(3)} } &= \\frac{\\partial \\mathcal L}{\\partial y} \\cdot \\frac{\\partial \\mathcal y}{\\partial z^{(3)}} = 2 (y-\\hat y) \\cdot S(z^{(3)})(1-S(z^{(3)})) \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial w ^ {(2)}} &=  \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial b ^ {(2)}} &=  \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial a ^ {(2)}} &=\\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial z ^ {(2)}} &= \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial w ^ {(1)}} &= \\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial b ^ {(1)}} &=\n",
    "\\end{align}\n",
    "\n",
    "As things get quite tricky with indices, we have implemented backpropagation for you in method $\\texttt{back_prop}$. Look at it and convince yourself that it generalizes the derivations above to the multi-dimensional case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting your model to MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have implemented your first neural network, and have understood backpropagation we will fit our model to the MNIST dataset.\n",
    "\n",
    "The MNIST database contains a total of 70,000 handwritten digits with 10 different classes, from 0 to 9. 60,000 examples are taken as training dataset and the left 10,000 as testing set. The digits have been size-normalized and centered in a fixed-size image. \n",
    "\n",
    "First download MNIST from: https://www.python-course.eu/data/mnist/mnist_train.csv\n",
    "and https://www.python-course.eu/data/mnist/mnist_test.csv\n",
    "and put these 2 csv files in the same folder as your jupyter notebook file.\n",
    "\n",
    "To make sure everything went fine, we will first load the dataset and visualize some samples from it. It might take a while to load it into memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "image_size = 28 # width and length\n",
    "no_of_different_labels = 10 #  i.e. 0, 1, 2, 3, ..., 9\n",
    "image_pixels = image_size * image_size\n",
    "train_data = np.loadtxt(\"mnist_train.csv\", \n",
    "                        delimiter=\",\")\n",
    "test_data = np.loadtxt(\"mnist_test.csv\", \n",
    "                       delimiter=\",\") \n",
    "\n",
    "x_train = np.asfarray(train_data[:, 1:])\n",
    "x_test = np.asfarray(test_data[:, 1:])\n",
    "y_train = np.asfarray(train_data[:, :1])\n",
    "y_test = np.asfarray(test_data[:, :1])\n",
    "# transform labels into one hot representation\n",
    "lr = np.arange(no_of_different_labels)\n",
    "y_train = (lr==y_train).astype(np.float)\n",
    "y_test = (lr==y_test).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the image and label from the training set\n",
    "for idx in range(5):\n",
    "    example_img = np.resize(x_train[idx],(28,28))\n",
    "    plt.imshow(example_img,cmap=\"Greys\")\n",
    "    plt.show()\n",
    "    print ('The digit is: ',y_train[idx].argmax())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to fit your model to MNIST, start by considering a simple one hidden layer MLP with the following specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions =  (28*28,     10,        10)\n",
    "activations = (      ReLU,  Sigmoid)\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Network class implemented above, intialize the network and fit it to the dataset using MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize  network structure\n",
    "nn = Network(dimensions, activations)\n",
    "# fit network to training data\n",
    "nn.fit(x_train, y_train, loss=MSE, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to previous exercise sessions, let's define a metric to measure the accuracy of our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction, ground_truth):\n",
    "    match = prediction.argmax(axis=1) == ground_truth.argmax(axis=1)\n",
    "    return float(np.sum(match)) / ground_truth.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then look at how our network is performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on training split\n",
    "prediction = nn.predict(x_train)\n",
    "print(\"Loss on TRAIN split: {}\".format(MSE.loss(prediction, y_train)))\n",
    "print(\"Accuracy on TRAIN split: {}\".format(accuracy(prediction, y_train)))\n",
    "\n",
    "print(\"------------------------\")\n",
    "\n",
    "# predict on testing split\n",
    "prediction = nn.predict(x_test)\n",
    "print(\"Loss on TEST split: {}\".format(MSE.loss(prediction, y_test)))\n",
    "print(\"Accuracy on TEST split: {}\".format(accuracy(prediction, y_test)))\n",
    "\n",
    "# visualize the image and label from the training set\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    idx = np.random.randint(0, x_test.shape[0])\n",
    "    example_img = np.resize(x_test[idx],(28,28))\n",
    "    plt.imshow(example_img,cmap=\"Greys\")\n",
    "    plt.show()\n",
    "    print ('Network prediction: ',prediction[idx].argmax())\n",
    "    print ('Ground truth annotation: ',y_test[idx].argmax())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the architecture, activations and hyperparameters to increase accuracy on the TEST split. You should be able to reach 96% accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
