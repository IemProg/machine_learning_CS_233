{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Session 11 - Non Linear Dimensionality Reduction\n",
    "\n",
    "Welcome to the 11th exercise session of CS233 - Introduction to Machine Learning.  \n",
    "\n",
    "We will explore **Autoencoder** for non linear dimensionality reduction. **Pytorch** will be used in this exercise, which can be reviewed in exercise 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a few packages\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder\n",
    "In the lecture, we learn about the concept of a non-linear encoder/decoder architecture, namely the convolutional autoencoder. An encoder $f_{e}$ first encodes the input $x$ into a feature $z$ and then this feature vector is decoded back to the input image by a decoder $f_{d}$, this process is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z = f_{e}(x) \\\\\n",
    "\\hat{x} = f_{d}(z)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The parameters in the autodecoder can be computed by minimizing the MSE loss:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_n \\lVert \\hat{x_n} - x_n \\rVert ^2 \\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **Conv2d** and **ConvTranspose2d** in pytorch to define our convolutional autoencoder in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST \n",
    "\n",
    "We first begin with a simple dataset -- MNIST which consists of gray images. In the training process, we visualize both the reconstructed image $\\hat{x}$ and the grayscaled encoded feature $z$ every 5 epoches, you could find the output pictures of last training batch in the folder named \"mnist_vis\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the mnist autoencoder network\n",
    "class autoencoder_mnist(nn.Module):\n",
    "    '''\n",
    "    Input size (1, 28, 28), then filling each layer according to its output size\n",
    "    \n",
    "    arg: d: int, the channel size of encoded feature\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, d=8):\n",
    "        super(autoencoder_mnist, self).__init__()\n",
    "        '''\n",
    "        Define an encoder with 3 convolutional layers with kernel size of 3, padding=1, \n",
    "        each one followed by a ReLU layer and a MaxPooling layer.\n",
    "        That is: 3 × (Conv2d --> ReLU --> MaxPool2d)\n",
    "        \n",
    "        Hint: use stride in pooling layers to reduce output size\n",
    "        \n",
    "        '''\n",
    "        self.encoder = nn.Sequential(\n",
    "            ### CODE HERE ###\n",
    "            nn.Conv2d(..., stride=1, padding=1),  # (16, 28, 28)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(...),  # (16, 14, 14)\n",
    "            nn.Conv2d(..., stride=1, padding=1),  # (8, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(...),  # (8, 7, 7)\n",
    "            nn.Conv2d(..., stride=1, padding=1),  # (d, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(...) # (d, 3, 3)\n",
    "        )\n",
    "        '''\n",
    "        Define a decoder with 3 transposed convolutional layers. The first two of them are followed by a ReLU layer.\n",
    "        That is: 2 × (ConvTranspose2d --> ReLU ) --> ConvTranspose2d \n",
    "        \n",
    "        Hint: use stride and proper kernel size to reconstruct the output size.\n",
    "        \n",
    "        '''\n",
    "        self.decoder = nn.Sequential(\n",
    "            ### CODE HERE ###\n",
    "            nn.ConvTranspose2d(...),   # (8, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(...),  # (16, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(...)   # (1, 28, 28)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.decoder(z)    \n",
    "        return x,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the folder to save the visualization pictures\n",
    "if not os.path.exists('./mnist_vis'):\n",
    "    os.mkdir('./mnist_vis/')\n",
    "    os.mkdir('./mnist_vis/reconstruction/')\n",
    "    os.mkdir('./mnist_vis/feature/')\n",
    "\n",
    "\n",
    "# define the hyper-parameters\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# get the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, ), (0.5, )),\n",
    "                             ])\n",
    "\n",
    "\n",
    "dataset = MNIST('mnist_data/', download=True, train=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64,num_workers=4, sampler=SubsetRandomSampler(range(12800)))\n",
    "\n",
    "# define a model and criterion\n",
    "model = autoencoder_mnist()\n",
    "### CODE HERE ###\n",
    "criterion = ...\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        # ===================forward=====================\n",
    "        ### CODE HERE ###\n",
    "        output,feature = ...\n",
    "        loss = ...\n",
    "        \n",
    "        # ===================backward====================\n",
    "        ### CODE HERE ###\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        \n",
    "    # log \n",
    "    print('Epoch [{}/{}], Training loss:{:.4f}'\n",
    "          .format(epoch+1, num_epochs, loss))\n",
    "    \n",
    "    if epoch==0 or (epoch+1) % 5 == 0:\n",
    "        pic_output = output        \n",
    "        save_image(pic_output, './mnist_vis/reconstruction/output_{}.png'.format(epoch+1))       \n",
    "        pic_feature = feature.view(feature.size(0), 1, 8, -1)\n",
    "        save_image(pic_feature, './mnist_vis/feature/feature_{}.png'.format(epoch+1), normalize=True) \n",
    "        \n",
    "        # visualization\n",
    "        img1 = mpimg.imread('./mnist_vis/reconstruction/output_{}.png'.format(epoch+1))\n",
    "        img2 = mpimg.imread('./mnist_vis/feature/feature_{}.png'.format(epoch+1))\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(121).set_title('Reconstruction')\n",
    "        plt.imshow(img1, aspect='auto')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(122).set_title('Encoded Feature')\n",
    "        plt.imshow(img2, aspect='auto')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "# save the trained network\n",
    "torch.save(model.state_dict(), './conv_autoencoder_mnist.pth')\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face \n",
    "\n",
    "In the previous exercise, we learn how to use PCA to generate the eigenfaces and reconstruct the face, here we show how to do the reconstruction with convolutional autoencoder.\n",
    "\n",
    "We visualize the reconstructed image $\\hat{x}$ and the grayscaled feature $z$ every 50 epoches in the folder named \"face_vis\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the face autoencoder network\n",
    "class autoencoder_face(nn.Module):\n",
    "    '''\n",
    "    Input size is (1, 64, 64), then filling each layer according to its output size\n",
    "    \n",
    "    arg: d: int, channel size of encoded feature\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, d=8):\n",
    "        super(autoencoder_face, self).__init__()\n",
    "        '''\n",
    "        Define an encoder with 3 convlotional layers with kernel size of 3, padding=1, \n",
    "        each one following by a ReLU layer and a MaxPooling layer.\n",
    "        That is: 3 × (Conv2d --> ReLU --> MaxPool2d)\n",
    "        \n",
    "        Hint: use stride in pooling layers to reduce output size\n",
    "        \n",
    "        '''\n",
    "        self.encoder = nn.Sequential(\n",
    "            ### CODE HERE ###\n",
    "            nn.Conv2d(..., stride=1, padding=1),  # (16, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(...),  #(16, 32, 32)\n",
    "            nn.Conv2d(..., stride=1, padding=1),  # (32, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(...),  # (32, 16, 16)\n",
    "            nn.Conv2d(..., stride=1, padding=1),  # (d, 16, 16)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(...) # (d, 8, 8)\n",
    "        )\n",
    "        '''\n",
    "        Define a decoder with 3 transposed convlotional layers. The first two of them is following by a ReLU layer.\n",
    "        That is: 2 × (ConvTranspose2d --> ReLU ) --> ConvTranspose2d \n",
    "        \n",
    "        Hints: Using stride and proper kernel size to reconstruct the output size.\n",
    "               Be careful about the kernel size in the first ConvTranspose2d layer.\n",
    "        \n",
    "        '''\n",
    "        self.decoder = nn.Sequential(\n",
    "            ### CODE HERE ###\n",
    "            nn.ConvTranspose2d(...),   # (32, 16, 16) \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(...),  # (16, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(...)   # (1, 64, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x = self.decoder(z)\n",
    "        return x,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the folder to save the visualization pictures\n",
    "if not os.path.exists('./face_vis'):\n",
    "    os.mkdir('./face_vis')\n",
    "    os.mkdir('./face_vis/reconstruction/')\n",
    "    os.mkdir('./face_vis/feature/')\n",
    "\n",
    "#define the hyper-parameters\n",
    "num_epochs = 500\n",
    "batch_size = 5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# load the dataset\n",
    "faces = fetch_olivetti_faces().data\n",
    "faces.resize((faces.shape[0],1, 64, 64))\n",
    "dataset = torch.from_numpy(faces)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#  try different values of d in the feature dimension\n",
    "d = 16\n",
    "\n",
    "# define a model and criterion\n",
    "model = autoencoder_face(d)\n",
    "### CODE HERE ###\n",
    "criterion = ...\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                             weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img = data\n",
    "        # ===================forward=====================\n",
    "        ### CODE HERE ###\n",
    "        output,feature = ...\n",
    "        loss = ...\n",
    "        \n",
    "        # ===================backward====================\n",
    "        ### CODE HERE ###\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        \n",
    "    # log\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch+1, num_epochs, loss))\n",
    "    if epoch==0 or (epoch+1) % 50 == 0:\n",
    "        pic_output = output\n",
    "        save_image(pic_output, './face_vis/reconstruction/output_{}.png'.format(epoch+1))\n",
    "        \n",
    "        pic_feature = feature.view(feature.size(0), 1, d*2, -1)\n",
    "        save_image(pic_feature, './face_vis/feature/feature_{}.png'.format(epoch+1), normalize=True)\n",
    "        \n",
    "        # visualization\n",
    "        img1 = mpimg.imread('./face_vis/reconstruction/output_{}.png'.format(epoch+1))\n",
    "        img2 = mpimg.imread('./face_vis/feature/feature_{}.png'.format(epoch+1))\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 4))\n",
    "        plt.subplot(121).set_title('Reconstruction')\n",
    "        plt.imshow(img1)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(122).set_title('Encoded Feature')\n",
    "        plt.imshow(img2)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# save the trained network\n",
    "torch.save(model.state_dict(), './conv_autoencoder_face.pth')\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can play with 'd' in PCA and the autoencoder to compare the reconstruction from PCA-eigenfaces with it from the autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Should encoder and decoder have similar architecture?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (other-env)",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
