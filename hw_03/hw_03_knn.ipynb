{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise session #3 - $k$-NN Classifier \n",
    "\n",
    "In this exercise you will implement  the **$k$-Nearest Neighbor classifier ($k$-NN)**. You will also get familiar with\n",
    "other very important concepts related to machine learning in practice,\n",
    "including data preprocessing, distance metrics, visualization, and model evaluation.\n",
    "\n",
    "We have provided general functionality and pointers for you here. Please complete the code with your own implementation below. Please also discuss and answer the follow-up questions.\n",
    "\n",
    "### 1. Dataset and problem description\n",
    "\n",
    "The Healthy Body dataset contains body measurements acquired from **1250 people _from different ages, genders, and nationalities_** from different hospitals around the world. Health professionals have performed medical examinations and classified the individuals into three different body categories: **underweight, normal weight, and overweight.**\n",
    "\n",
    "Our goal is to automate the role of the health professionals i.e, to predict the category of the new data . However, due to anonymity reasons, we have been provided access to limited information about the individuals: their measured _weights_ and _heights_, and their respective _body category_ only.\n",
    "\n",
    "We will use these features to train a $k$-NN classifier for the task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactive plots, so you can zoom/pan/resize plots\n",
    "%matplotlib notebook\n",
    "\n",
    "# Libraries for numerical handling and visualization. Install if required\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loading and visualization\n",
    "\n",
    "The goal of supervised classification algorithms such as $k$-NN is to use information from a set of labeled examples, i.e., examples for which we know their class assignments, to infer the classes for unlabeled examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "\n",
    "# Paths\n",
    "features_annotated_path = \"hbody_feats_annotated.npy\"     # Weights, heights of individuals with known body category\n",
    "labels_annotated_path = \"hbody_labels_annotated.npy\"      # Body categories of those individuals\n",
    "features_unannotated_path = \"hbody_feats_unannotated.npy\" # Weights and heights of unknown body category individuals\n",
    "                                                          # - Goal: Figure out their body categories\n",
    "\n",
    "# Features organized in an NxD matrix; N examples, D features.\n",
    "# Another way to look at it: each of the N examples is a D-dimensional feature vector.\n",
    "\n",
    "features_annotated = np.load(features_annotated_path)\n",
    "features_unannotated = np.load(features_unannotated_path)\n",
    "labels_annotated = np.load(labels_annotated_path)\n",
    "\n",
    "class_names = ('Underweight', 'Normal weight', 'Overweight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. What are the target variables? What are the predictor variables?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize annotated and unannotated sets\n",
    "\n",
    "colors = np.array([[0.85, 0.85, 0], [0, 0.5, 0], [0.25, 0.25, 1]])\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(f\"Annotated set ({len(labels_annotated)} examples)\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*features_annotated[labels_annotated==i].T,\n",
    "                c=colors[i, None], alpha=0.5, s=15, lw=0, label=class_name)\n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.ylabel(\"Height (cm)\")\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(f\"Unannotated set ({len(features_unannotated)} examples)\")\n",
    "plt.scatter(*features_unannotated.T, c='gray', alpha=0.5, s=15, lw=0, label='Unknown body category')\n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Do you think this is an easy or a difficult classification problem? Why?**  \n",
    "\n",
    "**Q. What should the unannotated set share in common with the annotated set?**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Normalizing data\n",
    "\n",
    "$k$-NN determines neighbors by computing the \"distance\" between two examples. For this process to work, we are required\n",
    "to normalize the features. This is true for many other machine learning algorithms as well.\n",
    "\n",
    "\n",
    "\n",
    "A very common way to normalize the data is by the so-called z-score standardization. It transforms values from an arbitrary range such that the result has mean $0$ and standard deviation $1$. The operation is defined as follows:\n",
    "\n",
    "$$\n",
    "x_{norm} = \\frac {x - \\mu_x} {\\sigma_x},\n",
    "$$\n",
    "for _each feature independently_.\n",
    "\n",
    "**Q. What would happen if we don't normalize the data?**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data.\n",
    "# Tip: Use numpy's broadcasting to write your normalization function\n",
    "\n",
    "def normalize(features, means, stds):\n",
    "    # WRITE YOUR CODE HERE\n",
    "    # return the normalized features\n",
    "    return \n",
    "\n",
    "# test normalization\n",
    "dummy_features = np.random.randint(100,size=(10,3))\n",
    "norm_dummy_features = normalize(dummy_features,dummy_features.mean(0,keepdims=True),dummy_features.std(0,keepdims=True))\n",
    "if np.allclose(norm_dummy_features.mean(axis=0), 0) and np.allclose(norm_dummy_features.std(axis=0), 1):\n",
    "    print(\"Everything alright here.\")\n",
    "else:\n",
    "    print(\"Nope. Try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The $k$-Nearest Neighbors Classifier\n",
    "\n",
    "$k$-NN assigns as label to a given example the most popular label from its surroundings. The method is very intuitive, and can be summarized as:\n",
    "- Compute the distance between the example to classify and all the training examples.\n",
    "- Select the closest $k$ training examples.\n",
    "- Assign to the example the most common label among those neighbors.\n",
    "\n",
    "### 4.1 Distance metrics\n",
    "\n",
    "There are many ways to define a distance between two examples. Two very common ones that we will use in this exercise are:\n",
    "\n",
    "#### Euclidean distance:\n",
    "\n",
    "$$\n",
    "m(\\mathbf{v}, \\mathbf{w}) = \\sqrt{ \\sum_{i=1}^d \\left(\\mathbf{v}_i - \\mathbf{w}_i\\right)^2 }\n",
    "$$\n",
    "\n",
    "This is the generalization of the Pythagorean theorem to an arbitrary number of dimensions, and corresponds to our intuitive interpretation of the straight-line distance between two points.\n",
    "\n",
    "#### Manhattan distance:\n",
    "\n",
    "$$\n",
    "m(\\mathbf{v}, \\mathbf{w}) = \\sum_{i=1}^d |\\mathbf{v}_i - \\mathbf{w}_i|\n",
    "$$\n",
    "\n",
    "Aggregates differences in features independently from one another. It is also known as city block distance. One can think of it as the minimum distance one would have to walk between two intersections in a city organized by regular blocks.\n",
    "\n",
    "\n",
    "**Q. Would you expect to find the same nearest neighbors to a point with both distance metrics?**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the euclidean distance between a vector and a collection of vectors (matrix)\n",
    "# Tip: numpy's broadcasting allows you to write this in a very intuitive and simple way.\n",
    "\n",
    "# example is a vector and training examples a matrix\n",
    "def euclidean_dist(example, training_examples):\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the manhattan distance between a vector and a collection of vectors (matrix)\n",
    "# Tip: numpy's broadcasting allows you to write this in a very intuitive and simple way.\n",
    "\n",
    "# example is a vector and training examples a matrix\n",
    "def manhattan_dist(example, training_examples):\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices of the k shortest distances from a list of distances\n",
    "#Tip: use np.argsort()\n",
    "\n",
    "def find_k_nearest_neighbors(k, distances):\n",
    "    # WRITE YOUR CODE HERE\n",
    "    indices = \n",
    "    return indices\n",
    "\n",
    "# test k nearest neighbors\n",
    "dummy_distances = [10.,0.5,200,0.006,43,4.5,11.,50]\n",
    "top_k = 5\n",
    "top_k_indices = find_k_nearest_neighbors(top_k,dummy_distances)\n",
    "\n",
    "if np.allclose(top_k_indices,[3,1,5,0,6]):\n",
    "    print('Implementation is correct')\n",
    "else:\n",
    "    print('Oops!! Something is wrong')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of neighbor labels, choose the most frequent one.\n",
    "# Tip: np.bincount and np.argmax are your friend.\n",
    "\n",
    "def predict_label(neighbor_labels):\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return \n",
    "\n",
    "# test label prediction\n",
    "dummy_labels = [10,3,2,10,2,2,2,10,10,11,1,2]\n",
    "if predict_label(dummy_labels) == 2:\n",
    "    print('Implementation is correct')\n",
    "else:\n",
    "    print('Oops!! Something is wrong')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 $k$-NN Step by Step for a Single Example\n",
    "Let's implement the algorithm for one example in this section. **Try out different values of k and observe the changes.**\n",
    "\n",
    "**Q. What do we use as mean and std to normalize our unknown sample? and Why?**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function you just defined to predict the label of a example\n",
    "\n",
    "# Choosing a random training example and treating as we don't know it's true label. \n",
    "total_samples = features_annotated.shape[0]\n",
    "dummy_unknown = np.random.choice(np.arange(total_samples),1)[0]\n",
    "all_indices = np.arange(total_samples)\n",
    "known_label = np.delete(all_indices,dummy_unknown)\n",
    "\n",
    "# Get the features corresponding to known and unknown points\n",
    "train_labels = labels_annotated[known_label]\n",
    "train_feats = features_annotated[known_label,:]\n",
    "dummy_unknown_feats =  features_annotated[dummy_unknown,:]\n",
    "\n",
    "# IMPORTANT: Normalize the data, what should be the mean and std?\n",
    "# WRITE YOUR CODE HERE\n",
    "mean_val = \n",
    "std_val = \n",
    "norm_train_feats = normalize(train_feats,mean_val,std_val)\n",
    "norm_dummy_unknown_feats = normalize(dummy_unknown_feats,mean_val,std_val)\n",
    "\n",
    "# choose number of neighbors\n",
    "k=6\n",
    "\n",
    "# find distance w.r.t all training examples\n",
    "distances = euclidean_dist(norm_dummy_unknown_feats, norm_train_feats)\n",
    "# find the nearest neighbors\n",
    "nn_indices = find_k_nearest_neighbors(k, distances)\n",
    "# find labels of nearest neighbors\n",
    "neighbor_labels = train_labels[nn_indices] \n",
    "# find the best label\n",
    "best_label = predict_label(neighbor_labels) \n",
    "\n",
    "print(f'Predicted label: {class_names[best_label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the unknown point and its neighbors.\n",
    "plt.figure()\n",
    "plt.title(f\"A randomly chosen unlabeled example from the validation set\\nand its {k}-nearest neighbors\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*norm_train_feats[train_labels==i].T,\n",
    "                c=colors[i, None], alpha=0.25, s=15, lw=0, label=class_name)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_indices = nn_indices[train_labels[nn_indices] == i]\n",
    "    if len(class_indices) > 0:\n",
    "        plt.scatter(*norm_train_feats[class_indices].T,\n",
    "                    c=colors[i, None], alpha=1, s=25, lw=0, label='Neighbor')\n",
    "\n",
    "ax = plt.scatter(*norm_dummy_unknown_feats[0], marker='*', c='brown', alpha=0.5, s=50, label='unlabeled example')\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Putting things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function kNN_one_example that applies all the previous steps\n",
    "# to predict the label of one example.\n",
    "\n",
    "def kNN_one_example(unlabeled_example, training_features, training_labels, k):\n",
    "    # WRITE YOUR CODE HERE\n",
    "    distances =                 # Compute distances\n",
    "    nn_indices =                # Find neighbors\n",
    "    neighbor_labels =           # Get neighbors' labels\n",
    "    best_label =                # Pick the most common\n",
    "    \n",
    "    return best_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function kNN that applies kNN_one_example to an arbitrary number of examples.\n",
    "# Tip: numpy's apply_along_axis does most of the work for you.\n",
    "\n",
    "def kNN(unlabeled, training_features, training_labels, k):\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. While the above implementation works, it has some drawbacks. Can you identify them?**  \n",
    "\n",
    "**Q. Can you think of a better implementation?**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Splitting the annotated data into training, validation, and test sets\n",
    "\n",
    "We need to ensure that our method generalizes, which means it will correctly predict the class for new provided examples.\n",
    "\n",
    "In order to simulate this scenario, we will split our annotated data into two groups: the training set, and the test set.\n",
    "- The **training set** will be used for finding a set of parameters and hyperparameters minimizing a given criterion.\n",
    "- The **test set** will be used for testing how well the learned model generalizes to data beyond that used for training.\n",
    "\n",
    "While the training set helps us find out how exactly to manipulate our data to find the right prediction, the test set tells us how well we expect to perform when given new data. Our training procedure is allowed to handle data from the training set only, and should not in any way use the information from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split labeled data into training and test set\n",
    "\n",
    "# Choose a fixed seed to reproduce same results\n",
    "np.random.seed(330)\n",
    "\n",
    "num_annotated_labels = len(labels_annotated)\n",
    "\n",
    "# percentage of annotated data for train and test sets. \n",
    "# what will be the implications of choosing different ratios of train and test?\n",
    "training_perc, test_perc = 0.8, 0.2\n",
    "\n",
    "indices = np.arange(len(labels_annotated))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "num_train_samples = int(training_perc*num_annotated_labels)\n",
    "num_test_samples = num_annotated_labels-num_train_samples\n",
    "\n",
    "# split indices into train and test folds\n",
    "training_indices, test_indices = np.split(indices,indices_or_sections=[num_train_samples])\n",
    "\n",
    "# Get the training and test data and labels\n",
    "training_features = features_annotated[training_indices]\n",
    "training_labels = labels_annotated[training_indices]\n",
    "test_features = features_annotated[test_indices]\n",
    "test_labels = labels_annotated[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features of train and test data with mean 0 and std 1\n",
    "# WRITE YOUR CODE HERE\n",
    "mean_features = \n",
    "std_features = \n",
    "norm_train_features = normalize(training_features,mean_features,std_features)\n",
    "norm_test_features = normalize(test_features,mean_features,std_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 K-Fold Cross-validation\n",
    "\n",
    "If we are only allowed to assess our model generalization _after_ training, how can we monitor and guide the training process? How can we know beforehand that we are using the best version of our model?\n",
    "\n",
    "The most common strategy for this, called **[cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))**, is, simply put, to pretend that a part of our training data is in fact unannotated, and see if our method manages to predict the annotations correctly. In other words, we will reserve a portion of our training data to temporarily act as a small test set. \n",
    "\n",
    "Cross Validation(CV) technique is used to choose the best parameters for separate validation set, where the hope is that this set will be close to the (unseen) test set. We train our model on train set and use hyperparameters which give best results on validation set. This is a general technique to choose your best machine learning model.\n",
    "\n",
    "K-fold is a type of CV technique that we'll see here. We'll split data in K parts and use kth part for validation, whereas k-1 for training. This process will be repeated K times. Metric used to evaluate the model is accuracy on the validation set.\n",
    "\n",
    "Splitting your training data into a training and a validation set is also used for comparing different \"versions\" of your model. Many machine learning methods depend on predefined configuration settings, called _hyperparameters_, that heavily influence how the method behaves. In the case of $k$-NN, one such parameter is $k$, the number of neighbors.\n",
    "\n",
    "In this exercise, we will be applying k-fold cross validation. Technically, your validation set is part of your training data. To avoid confusion, however, we often call the _training set_ the portion of your training data used for tuning your method. Therefore, for k-fold cross validation, we  split our annotated data into a training data, and a test set. Further, we split the training data into k-folds from which one fold is used for validation set and rest of the folds as training set. This process is repeated K times and average performance is calculated. We choose the value of hyperparameter which minimises the error on the cross-validations set. \n",
    "\n",
    "\n",
    "**Q. What is the difference between validation set and test set?**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data indices\n",
    "\n",
    "# Inputs:\n",
    "#        num_examples: total samples in the dataset\n",
    "#        k_fold: number fold of CV\n",
    "# \n",
    "# Output: array of shuffled indices with shape (k_fold, num_examples//k_fold)\n",
    "\n",
    "def fold_indices(num_examples,k_fold):\n",
    "    ind = np.arange(num_examples)\n",
    "    split_size = num_examples//k_fold\n",
    "     \n",
    "    k_fold_indices = []\n",
    "\n",
    "    # WRITE YOUR CODE HERE\n",
    "    # Generate k_fold set of indices\n",
    "    k_fold_indices = \n",
    "         \n",
    "    return np.array(k_fold_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. What does `np.random.seed` do? Why is this useful?**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Performance metric to know if the  prediction is good?\n",
    "\n",
    "In order to quantify the performance of our model, we want to obtain a score that tells us how close the predictions were to the expected classification.\n",
    "\n",
    "The simplest way to do this is to compute the ratio of correctly predicted examples, also known as the accuracy:\n",
    "\n",
    "$$\n",
    "\\frac 1 N \\sum_{n=1}^N \\mathbf{1}[\\hat{y} = y]\n",
    "$$\n",
    "**Q. Do you see any limitation to using accuracy to evaluate your model?**\n",
    "\n",
    "\n",
    "\n",
    "**Q. Can you think of other ways to evaluate your model?**\n",
    "\n",
    "\n",
    "**Q. Is accuracy suitable for multiclass classification?**  \n",
    "\n",
    "**Q. What other criteria, aside from accuracy, should one consider when choosing hyperparameters?**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that computes the accuracy between a predicted and the expected labels.\n",
    "def accuracy(predicted, target):\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN function using kth split as validation set and rest k-1 splits to train model. \n",
    "\n",
    "#Inputs:\n",
    "#   k - kth fold as cross validation split\n",
    "#   k_fold_ind -  indices of each split of k-fold split\n",
    "#   X  -  training data\n",
    "#   Y  - train labels\n",
    "#   NN - number of nearest neighbours for k-NN algorithm\n",
    "\n",
    "#Outputs:  return the accuracy of validation set\n",
    "\n",
    "def kNN_CV(k_fold,k_fold_ind,X,Y,k ):\n",
    "    \n",
    "    #use kth split for validation\n",
    "    val_ind = k_fold_ind[k_fold]\n",
    "    \n",
    "    # use rest k-1 splits to train\n",
    "    train_splits = [i for i in range(k_fold_ind.shape[0]) if i is not k_fold]\n",
    "    train_ind = k_fold_ind[train_splits,:].reshape(-1)\n",
    "    \n",
    "    #Get train and val \n",
    "    X_train = X[train_ind,:]\n",
    "    Y_train = Y[train_ind]\n",
    "    X_val = X[val_ind,:]\n",
    "    Y_val = Y[val_ind]\n",
    "    \n",
    "    \n",
    "    Y_val_pred = kNN(X_val, X_train, Y_train, k)\n",
    "    \n",
    "    #get accuracy for val\n",
    "    acc = accuracy(Y_val_pred, Y_val)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a 4-fold cross validation on training dataset\n",
    "\n",
    "k =  4  # number of nearest neighbours for k-NN\n",
    "k_fold = 4\n",
    "k_fold_ind = fold_indices(norm_train_features.shape[0],k_fold)\n",
    "acc = np.zeros(k_fold)\n",
    "for fold in range(k_fold):\n",
    "    # Get validation accuracy with kth fold as validation set and rest as training set\n",
    "    # WRITE YOUR CODE HERE\n",
    "    acc[fold] = kNN_CV(...)\n",
    "mean_acc = \n",
    "\n",
    "print(f\"{k}-NN Classifier predicted correctly  with accuracy of {mean_acc:.2%} on cross-validation set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q. How should one select the number of folds of the cross-validation set?**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Hyperparameter optimization\n",
    "\n",
    "Did we choose the best $k$?\n",
    "\n",
    "We can evaluate our model under different values of $k$ to compare them. A simple way to do it is to evaluate our model's predictions for the same validation set when using different values of $k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the CV with different k values\n",
    "\n",
    "def run_cv_for_hyperparam(k_values, k_fold,norm_train_features,training_labels):\n",
    "\n",
    "    k_fold_ind = fold_indices(norm_train_features.shape[0],k_fold)\n",
    "    model_performance = []  # Store the computed metrics here\n",
    "    for k in k_values:\n",
    "        for fold in range(k_fold):\n",
    "            # WRITE YOUR CODE HERE\n",
    "            acc[fold] = kNN_CV(...)\n",
    "        mean_acc = \n",
    "        model_performance.append(mean_acc)\n",
    "        \n",
    "    return model_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the performances for different values of k\n",
    "\n",
    "k_values = range(1, 30)          # Try different values for hyperparameter k\n",
    "k_fold = 4\n",
    "model_performance= run_cv_for_hyperparam(k_values,k_fold,norm_train_features,training_labels)\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.title(\"Performance on the validation set for different values of $k$\")\n",
    "plt.plot(k_values, model_performance)\n",
    "plt.xlabel(\"Number of nearest neighbors $k$\")\n",
    "plt.xticks(k_values)\n",
    "plt.ylabel(\"Performance (accuracy)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick hyperparameter value that yields the best performance\n",
    "# WRITE YOUR CODE HERE\n",
    "best_k = \n",
    "\n",
    "print(f\"Best number of nearest neighbors on validation set is k={best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. How do you expect the model to perform with large k values equal to number of training example ?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the performances for different values of k\n",
    "\n",
    "k_values = range(1, 500, 30)          # Try different values for hyperparameter k\n",
    "k_fold = 4\n",
    "model_performance= run_cv_for_hyperparam(k_values,k_fold,norm_train_features,training_labels)\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.title(\"Performance on the validation set for different values of $k$\")\n",
    "plt.plot(k_values, model_performance)\n",
    "plt.xlabel(\"Number of nearest neighbors $k$\")\n",
    "plt.xticks(k_values)\n",
    "plt.ylabel(\"Performance (accuracy)\");\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Does your final model generalize to Unseen test data?\n",
    "\n",
    "Now that we have tuned our model, we can apply it for prediction on the test set using the optimal $k$ found on cross-validations set.\n",
    "\n",
    "**Q. How do you expect the model to perform, compared with the cross-validation set performance?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels on the test-set using the best k value after CV\n",
    "# WRITE YOUR CODE HERE\n",
    "predicted_test_labels = kNN(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the predictions on the test set\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Predicted classes for the test set\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*norm_train_features[training_labels==i].T,\n",
    "                c=colors[i, None], alpha=0.1, s=15, lw=0)\n",
    "for i, class_name in enumerate(class_names):    \n",
    "    plt.scatter(*norm_test_features[predicted_test_labels==i].T,\n",
    "                c=colors[i, None], marker='*', alpha=0.5, s=50, lw=0, label=class_name)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = accuracy(test_labels, predicted_test_labels)\n",
    "print(f\"{best_k}-NN Classifier predicted correctly {performance:.2%} of the test examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Was this the value you were expecting?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predicting on unannotated data\n",
    "\n",
    "We are finally ready to apply our model for prediction on unannotated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preparation for unannotated data\n",
    "features_unannotated = np.load(features_unannotated_path)\n",
    "# WRITE YOUR CODE HERE\n",
    "norm_features_unannotated = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. What should one take into account when feeding new data to a machine learning model?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted labeles for unannotated data.\n",
    "# WRITE YOUR CODE HERE\n",
    "predicted_labels_unannotated = kNN(..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize the predictions on the unannoated set\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Predicted classes for unannotated data\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*norm_train_features[training_labels==i].T,\n",
    "                c=colors[i, None], alpha=0.1, s=15, lw=0)\n",
    "for i, class_name in enumerate(class_names):    \n",
    "    plt.scatter(*norm_features_unannotated[predicted_labels_unannotated==i].T,\n",
    "                c=colors[i, None], marker='*', alpha=0.5, s=50, lw=0, label=class_name)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Do these class assignments look reasonable to you?**  \n",
    "\n",
    "\n",
    "\n",
    "**Q. How would you evaluate if your predictions are reasonable here, without labels?**  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
