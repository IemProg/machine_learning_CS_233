{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: Dimensionality Reduction\n",
    "In this exercise, we will implement and see the working of a dimensionality reduction techniques Prinical Component Analysis (PCA) and Linear Discriminant Analysis (LDA) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good to import few packages\n",
    "%matplotlib notebook\n",
    "from ipywidgets import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Toy Dataset\n",
    "Let see the PCA results on a toy iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "data = iris['data'].astype(np.float32) \n",
    "labels = iris['target'] \n",
    "cls_names = iris['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the trends of different features together. One can see that one class is well separated from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "count = 1\n",
    "colors = np.array([[0.85, 0.85, 0], [0, 0.5, 0], [0.25, 0.25, 1]])\n",
    "for i in range(3):\n",
    "    for j in range(i+1,4):\n",
    "        plt.subplot(2,3,count)\n",
    "        for ind,name in enumerate(cls_names):\n",
    "            filtered_class = labels==ind\n",
    "            plt.scatter(data[filtered_class,i],data[filtered_class,j],c=colors[ind,None],label=name)\n",
    "        plt.xlabel(f'feature_{i}')\n",
    "        plt.ylabel(f'feature_{j}')\n",
    "        plt.legend()\n",
    "        count +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA\n",
    "In the above dataset, we have 4 features per data point and now let's try to reduce it dimensionality to 2 using PCA. Implement the following PCA function.\n",
    "\n",
    "\n",
    "For a dataset $\\mathbf{X}\\in R^{N\\times D}$, PCA solves follwing optimization problem\n",
    "   \\begin{align}\n",
    "    \\underset{\\mathbf{W}}{\\operatorname{max}} \\mathbf{W}^T\\mathbf{C}\\mathbf{W}\\\\\n",
    "    s.t. ~~~~~ \\mathbf{W}^T\\mathbf{W} = \\mathbf{I_d}\n",
    "   \\end{align}\n",
    "   \n",
    " where data convariance matrix\n",
    "     \\begin{align}\n",
    "         \\mathbf{C} &= \\frac{1}{N}\\sum_{i=0}^{N-1}(\\mathbf{x_i}-\\mathbf{\\bar{x}})(\\mathbf{x_i}-\\mathbf{\\bar{x}})^T\\\\\n",
    "         \\mathbf{\\bar{x}} &= \\frac{1}{N}\\sum_{i=0}^{N-1} \\mathbf{x_i}\n",
    "     \\end{align}\n",
    "     \n",
    " and $\\mathbf{W}\\in R^{D\\times d}$, $\\mathbf{x}\\in R^{D\\times 1}$, $\\mathbf{\\bar{x}}\\in R^{D\\times 1}$\n",
    " \n",
    " The solution to this problem is finding eigenvectors for $d(<D)$ largest eigenvalues of data covariance matrix $\\mathbf{C}$. Hence, $\\mathbf{W}$ is a matrix of $d$ eigenvectors each being $D$-dimensional. \n",
    " \n",
    "We project our original data $\\mathbf{X}\\in R^{N\\times D}$ space to $R^{N\\times d}$, using centered data $\\mathbf{\\tilde{X}}\\in R^{N\\times D}$,\n",
    "    \\begin{align}\n",
    "        \\mathbf{\\hat{X}} &= \\mathbf{\\tilde{X}}\\mathbf{W} \\\\\n",
    "        \\mathbf{\\tilde{X}(i)} &= \\mathbf{x_i}-\\mathbf{\\bar{x}} ~~~~ i\\in \\[ 0,N-1\\]\n",
    "    \\end{align}\n",
    " \n",
    "To understand how much of variance is explained by our $d$ eigenvectors, we compute percentage of variance explained by \n",
    "    \\begin{align}\n",
    "        \\mathbf{exvar} = \\frac{\\sum_{i=0}^{d-1}\\lambda_i}{\\sum_{i=0}^{D-1}\\lambda_i}\n",
    "    \\end{align}\n",
    "where $\\lambda_i$ is the ith largest eigenvalue. For different applications, you would like to choose $d$ such that explained variance is greater than a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input:\n",
    "    X: NxD matrix representing our data\n",
    "    d: Number of principal components to be used to reduce dimensionality\n",
    "    \n",
    "Output:\n",
    "    mean_data: 1xD representing the mean of input data\n",
    "    W: Dxd principal components\n",
    "    eg: d values representing variance corresponding to principal components\n",
    "    X_hat: Nxd data projected in principal components' direction\n",
    "    exvar: explained variance by principal components\n",
    "'''\n",
    "def PCA(X, d):\n",
    "    \n",
    "    # Compute the mean of data\n",
    "    mean = \n",
    "    # Center the data with the mean\n",
    "    X_tilde = \n",
    "    # Create covariance matrix\n",
    "    C = \n",
    "    # Compute eigenvector and eigenvalues. Hint use: np.linalg.eigh\n",
    "    eigvals, eigvecs = \n",
    "    # Choose top d eigenvalues and corresponding eigenvectors. Sort eigenvalues( with corresponding eigenvector )\n",
    "    # in decreasing order first.\n",
    "    eigvals =\n",
    "    eigvecs = \n",
    "\n",
    "    W = eigvecs[:, 0:d]\n",
    "    eg = eigvals[0:d]\n",
    "\n",
    "    # project the data using W\n",
    "    X_hat = \n",
    "    \n",
    "    #explained variance\n",
    "    exvar = \n",
    "\n",
    "    return mean, W, eg, X_hat, exvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the implemented function and visualize the projected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "mean, W, eg, X_hat, exvar = PCA(data, d)\n",
    "print(f'Total Variance explained by first {d} pc is {exvar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for ind,name in enumerate(cls_names):\n",
    "    filtered_class = labels==ind\n",
    "    plt.scatter(X_hat[filtered_class,0],X_hat[filtered_class,1],c=colors[ind,None],label=name)\n",
    "plt.xlabel(f'feature_0')\n",
    "plt.ylabel(f'feature_1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What happens when d=D?  \n",
    "\n",
    "**Q.** What happens when D>>N?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EigenFaces\n",
    "Now, we will use PCA on face of images. The goal is to represent faces in the dataset with set of faces, called eigenfaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces().data\n",
    "print(f'Dimensions of Face dataset N={faces.shape[0]}, D={faces.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different values of d and see the variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 30\n",
    "mean, W, eg, X_hat, exvar = PCA(faces, d)\n",
    "print(f'Total Variance explained by first {d} pc is {exvar}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualize\n",
    "Let see what do these principal component look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(mean.reshape(64,64),cmap='gray')\n",
    "plt.title('Mean Face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# see first 10 principal components\n",
    "plt.figure(figsize=(8,18))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.imshow(W.reshape(64,64,-1)[...,i],cmap='gray')\n",
    "    plt.xlabel(f'Principal Component:{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what these components account for. Vary the slider to change the principal component and their influence on the mean value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "line = ax.imshow(mean.reshape(64,64),cmap='gray')\n",
    "\n",
    "def update(pcind = 0,pcweight=0):\n",
    "    img = W.copy()[:,pcind]*pcweight\n",
    "    line.set_data((img+mean).reshape(64,64))\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "interact(update,pcind=(0,d-1,1),pcweight=(-10,10,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** Can you identify what component accounts for what?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Reconstruction\n",
    "We project our original data to smaller dimension and depending upon how many dimension are kept, we will have some loss of information. Here we will see how changing final number of dimensions our reconstruction is affected. Also relate this to the total variance explained by the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different values of d\n",
    "d = 10\n",
    "mean, W, eg, X_hat, exvar = PCA(faces, d)\n",
    "print(f'Total Variance explained by first {d} pc is {exvar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = np.random.choice(faces.shape[0],1)[0]\n",
    "sample_face = faces[sample_id,:]\n",
    "#project this face in smaller dimension. remember to center the data first\n",
    "proj_face = \n",
    "#bring to back to original space\n",
    "reconstructed_face = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.imshow(sample_face.reshape(64,64),cmap='gray')\n",
    "ax.set_title('Original Image')\n",
    "ax = plt.subplot(1,2,2)\n",
    "plt.imshow(reconstructed_face.reshape(64,64),cmap='gray')\n",
    "ax.set_title('Reconstructed Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fisher Linear Discriminant Analysis\n",
    "This supervised method is used to reduce dimensionality along with learning a projection, which keeps data points belonging to same class together. We will use sklearn's implementation of the Fisher LDA to project MNIST data to smaller dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST data\n",
    "mnist = datasets.load_digits()\n",
    "data = mnist.data\n",
    "labels = mnist.target\n",
    "num_class = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project to 2 dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# see the documentation of the method to understand the parameters\n",
    "d = 2\n",
    "clf = LDA(n_components=d)\n",
    "# call the fit function\n",
    "obj = \n",
    "# computed the variance explained using clf's parameter\n",
    "exvar = \n",
    "print(f'Variance explained {exvar}')\n",
    "proj = obj.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "colors = cm.jet(np.linspace(0, 1, num_class))\n",
    "for i in range(num_class):\n",
    "    inds = labels == i\n",
    "    plt.scatter(proj[inds,0],proj[inds,1],c=colors[i])\n",
    "plt.legend(np.arange(num_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projection in 3 dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3\n",
    "clf = LDA(n_components=d)\n",
    "# call the fit function\n",
    "dd = \n",
    "proj = dd.transform(data)\n",
    "# computed the variance explained using clf's parameter\n",
    "exvar = \n",
    "print(f'Variance explained {exvar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "for i in range(num_class):\n",
    "    inds = labels == i\n",
    "    ax.scatter3D(proj[inds,0], proj[inds,1], proj[inds,2],c=colors[i])\n",
    "plt.legend(np.arange(num_class))\n",
    "plt.title('Drag the plot see the clusters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
